# Global parameters
systemFiles           = 100CH3OH-CG.in, 60CH3OH-CG.in, 40CH3OH-CG.in, 10CH3OH-CG.in   # List of input filenames for each system
symmetryFunctionFile  = symmetry-functions.in       # Symmetry function file 
mode                  = training                    # Run MC sampling (simulation) or ML-IMC optimization (training)
outputMode            = default                     # default (rdf, energy, model, opt, gradients); verbose (+trajectories)  
modelFile             = methanol-CG-NN.bson                        # Specify a model filename, "none" for random initialization 
gradientsFile         = none                        # Specify a gradients filename, "none" for default initialization
optimizerFile         = none                        # Specify an opt filename, "none" for default initialization
adaptiveScaling       = false                       # Scales gradients based on system loss (true) or uniform averaging (false)
consectuiveGradients  = true                        # Applies gradients from one system at a time (true) or averages from different systems (false)
                                  
# Monte Carlo parameters
steps          = 2000000                # Total number of MC steps
Eqsteps        = 500000                 # Equilibration MC steps
stepAdjustFreq = 5000                   # Frequency of MC step adjustment   
trajout        = 5000                   # XTC trajectory frequency
outfreq        = 1000                   # Output frequency

# Network parameters
neurons         = 20, 20, 20, 1                                 # Number of hidden neurons in the network (plus an output neuron)
iters           = 25                                # Number of learning iterations
activations     = identity, relu, relu, identity                          # Activation functions
REGP            = 0.0                               # Regularization parameter (set to zero to turn off regularization)
optimizer       = AMSGrad                           # Optimizer type (Recommended: AdaBelief, AMSGrad, Adam)
                                                    # for more info: https://fluxml.ai/Flux.jl/stable/training/optimisers/
rate            = 0.00025                              # Learning rate
momentum        = 0.9                               # Momentum / Nesterov Momentum coefficient
decay1          = 0.9                               # Decay of momentum (decay_1, decay_2)
decay2          = 0.999

# Pre-training parameters
PTsteps         = 50000         # Number of pre-training steps
PToutfreq       = 100           # Frequency of pre-training reporting
PTREGP          = 0.0           # Regularization parameter (set to zero to turn off regularization)
PToptimizer     = AMSGrad
PTrate          = 0.00025
PTmomentum      = 0.9
PTdecay1        = 0.9
PTdecay2        = 0.999
